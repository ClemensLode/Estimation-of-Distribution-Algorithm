\documentclass[12pt, a4paper]{scrartcl}

\usepackage{graphicx}
\usepackage{dsfont}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{tweaklist}
\pagestyle{headings}

\renewcommand{\figurename}{\footnotesize{Fig.}}
\renewcommand{\enumhook}{\setlength{\topsep}{-10pt}\setlength{\itemsep}{0pt}}
\renewcommand{\labelenumi}{(\Roman{enumi})}
\renewcommand{\labelenumii}{(\Roman{enumi}.\Roman{enumii})}
\renewcommand{\labelenumiii}{(\alph{enumiii})}
\renewcommand{\labelenumiv}{(\arabic{enumiv})}
\newcommand{\mycaption}[1]{\caption{\footnotesize{#1}}} 

\renewcommand{\topfraction}{0.9}
\renewcommand{\bottomfraction}{0.8}
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     
\setcounter{dbltopnumber}{2}
\renewcommand{\dbltopfraction}{0.9}
\renewcommand{\textfraction}{0.07}
\renewcommand{\floatpagefraction}{0.7}
\renewcommand{\dblfloatpagefraction}{0.7}
\addtolength{\abovecaptionskip}{-0.1in}
\addtolength{\belowcaptionskip}{-0.1in}

\author{Clemens Lode}

\begin{document}

\thispagestyle{empty}
\noindent
\begin{tabular}{ll}
  \hspace{-2mm}
  \parbox[b]{0.88\textwidth}
  {
    \flushleft
    Institut für Angewandte Informatik\\
    und Formale Beschreibungsverfahren\\
    Universität Karlsruhe (TH)\\
    \vspace{1em}

    Forschungsgruppe Effiziente Algorithmen\\
    Prof. Dr. Hartmut Schmeck\\
  } & 
\includegraphics[height=7em]{AIFB_Logo_senkrecht.png}\\

\end{tabular}

\vspace{2cm}
\begin{center}{\bf \LARGE Studienarbeit}\\
\vspace{2cm}
{\bf "Reducing diversity loss in estimation of distribution algorithms"}\\
\vspace{1cm}
von
\vspace{1cm}
{\bf Clemens Lode}\\
\vspace{1cm}
eingereicht am 15.11.2006\\
beim Institut f\"ur Angewandte Informatik\\
und Formale Beschreibungsverfahren (AIFB)\\
der Universit\"at Karlsruhe (TH)\\
\vspace{2cm}
Betreuer:	&J\"urgen Branke\\
Referent:	&Prof. Dr. H. Schmeck\\
\vspace{2cm}
\begin{tabbing}
xxxxxxxxxxxxxxxx\=xxxxxxxxxxxxxxxx\kill
Heimatanschrift:\>Studienanschrift:\\
Edelwei\ss{}weg 16\>L\"otzener Stra\ss{}e 16\\
87493 Lauben\>76139 Karlsruhe\\
clemens@lode.de\\
\end{tabbing}

\end{center}
\newpage
\vspace{20cm}
Ich versichere hiermit wahrheitsgem\"a\ss{}, die Arbeit bis auf die dem Aufgabensteller bereits bekannte Hilfe selbstst\"andig angefertigt, alle benutzten Hilfsmittel vollst\"andig und genau angegeben und alles kenntlich gemacht zu haben, was aus Arbeiten anderer unver\"andert oder mit Ab\"anderungen entnommen wurde.\\
\\
\\
Karlsruhe, den 15. November 2006\\
\newpage
\newpage

\tableofcontents

\newpage

\section{Introduction}

Many Estimation of Distribution Algorithms (EDAs) can reach a state where the probability of ever finding the optimum is zero which is due to diversity loss. Diversity of a population denotes the ratio of different values in the population, i.e. if uniformity in all components is reached the diversity of this population is zero. If a different set of values is required for the optimal solution the optimum will never be sampled. If no action is taken to either prevent uniformity of the values in a single component or to reduce the diversity loss in each generation, the diversity can never be restored or held at a certain level and continuing the calculation will not increase the probability of finding the optimal or even a better solution.\\
In \cite{Shapiro} it was shown that the expected factor of diversity loss of selecting \(N\) individuals from a population of the size \(M\) and generating a new population of the size \(M\) is \(1 - \frac{1}{N}\) on a flat fitness landscape. It was shown that this is true for a whole class of EDAs where the distribution vector \(p\) is build using only data sampled from the last generation, the population and selection size \(M\) and \(N\) is constant and the new generation is created randomly on basis of the distribution vector.\\
One way to counter this diversity loss is to increase the population size, by checking and correcting the population if uniformity in a component is reached or by using the Laplace correction. This report will use a different approach by adjusting the distribution vector according to the population size, selection size and generation method.\\
In this report it will be shown that the factor of diversity loss of randomly generating a new population of the size \(M\) is \(1 - \frac{1}{M}\) and it will be calculated that the diversity loss of selecting \(N\) individuals can be reverted with the factor \(x = \frac{N(M-1)}{M(N-1)}\) and it will be shown that the resulting correction method improves the fitness and diversity on different fitness landscapes.\\
In addition, instead of randomly creating a new generation by applying the distribution vector on each single component of all individuals of the population, distributing exactly \(p \cdot M\) '1's in the population will result in a total loss of diversity of only \(\frac{M(N-1)}{N(M-1)}\) instead of \(1 - \frac{1}{N}\). It will be shown that reverting the loss of diversity will result in better performance with complex problems. The resulting method corrects \(p\) to \(\frac{1}{2}(1 - \sqrt{1 - x)}\) for \(p < \frac{1}{2}(1 - \sqrt{1 - \frac{1}{x}})\), to \(\frac{1}{2}(1 + \sqrt{1 - x})\) for \(p > \frac{1}{2}(1 + \sqrt{1 - \frac{1}{x}})\) and to \(\frac{1}{2}\) otherwise.\\
In this report at first we will define the expected diversity of a population (Chapter 2), then calculate the loss of diversity of selecting \(N\) individuals and generating \(M\) individuals and how to revert it (Chapter 3), then examine the distribution of exactly \(p \cdot M\) '1's in the population and how rounding errors are a problem. In the second part four fitness landscapes are defined (Chapter 5), optional methods that prevent \(p\) from containing extreme values are discussed (Chapter 6) and the test configuration is explained (Chapter 7). Finally various methods are compared and discussed (Chapter 8) and the results concluded (Chapter 9).

\newpage
\section{Definitions}

Let
\begin{itemize}
\addtolength{\itemsep}{-0.5\baselineskip}
\item \(C\): number of components of each individual
\item \(N\): size of the population
\item \(n\): number of selected individuals
\item \(A\): set of different values a component can take
\item \(|A|\): number of different values a component can take
\item \(x^{\mu}_{i}\): value of component i of individual \(\mu\)
\item \(\varphi(x)\): '1' if the condition x is met, '0' otherwise
\end{itemize}

\subsection{Variance of a population}

\(v^{a}_{i}\) describes the ratio of a certain value of a component in the population. E.g. \(v^{0}_2 = 0.3\) means that 30\% of all the individuals in the population have a '0' in the second component. 
\begin{equation}
\label{eq:variance1}
v^{a}_{i} = \frac{1}{N} \sum_{\mu=1}^{N} \varphi(x^{\mu}_{i} = a)
\end{equation}

\(v_{i}\) denotes the average variance of all values of a certain value of a component in the population.
\begin{equation}
\label{eq:variance2}
v_{i} = \frac{1}{|A|} \sum_{a=0}^{|A|-1} v^{a}_{i} (1 - v^{a}_{i})
\end{equation}

\(v\) denotes the sum of the average variances of all values of the values in all components in the population.
\begin{equation}
\label{eq:variance3}
v = \frac{1}{|A|} \sum_{i=1}^{C} \sum_{a=0}^{|A|-1} v^{a}_{i} (1 - v^{a}_{i})
\end{equation}

The scope of this report is to apply Univariate Marginal Distribution Algorithms ({\bf UMDA}s) on problems with a flat fitness landscape. UMDAs assume independence between variables, i.e. we examine each component independently and set \(C = 1\). In addition the UMDAs we will discuss here are represented as bit-strings, i.e. \(|A| = 2\).\\

As defined above, \(v\) denotes the sum of the average variances of all values of all components in the population. With \(|A| = 2\) and with \(k = (k_{1}, k_{2}, ..., k_{C})\), with \(k_{i}\) denoting the number of '1's in the \(i\)th component, we get for equation~\ref{eq:variance3}:
\[
v = \frac{1}{2} \sum_{i=1}^{C} \sum_{a=0}^{1} v^{a}_i (1 - v^{a}_i)
\]
and for a given \(k\) we can calculate the \(v^{a}_i\)'s 
\[
v^{a=0}_{i} = \frac{M - k_{i}}{M}
\]
\[
v^{a=1}_{i} = \frac{k_{i}}{M}
\]
This is true because \(\sum_{\mu=1}^{M} \varphi(x^{\mu}_i = 1) = k_{i}\) and \(\sum_{\mu=1}^{M} \varphi(x^{\mu}_i = 0) = (M - k_{i})\).\\

So we get:

\[
v = \frac{1}{2} \sum_{i=1}^{C} \sum_{a=0}^{1} v^{a}_i (1 - v^{a}_i) = \frac{1}{2} \sum_{i=1}^{C} [v^{0}_i (1-v^{0}_i) + v^{1}_i (1 - v^{1}_i)] =
\]
\[
      \frac{1}{2} \sum_{i=1}^{C} [\frac{M - k_i}{M} (1 - \frac{M-k_i}{M}) + \frac{k_i}{M} (1 - \frac{k_i}{M})] = {\bf \sum_{i=1}^C \frac{k_i M - k^{2}_{i}}{M^{2}}}
\]
And
\begin{equation}
\label{eq:variance4}
	v_{i} = \frac{k_i M - k^{2}_{i}}{M^{2}}
\end{equation}

\subsection{Random Distribution with UMDAs (RD)}

With UMDAs we determine in each step the ratio of '1' for each component of the selected part of the population and calculate the ratio. In the literature (see \cite{Muehlenbein}) this is usually called \(p_{i}\) and it is equal to our definition of \(v^{a=1}_{i}\), so we set \(p_{i} := v^{1}_{i}\). The distribution vector is called \(p := {p_{1}, p_{2}, ..., p_{C}}\).\\
From this distribution vector \(p\) a new population of the size \(M\) is generated where each individual has a '1' in its \(i\)th component with the probability \(p_{i}\) (or likewise a '0' with the probability \((1 - p_{i})\)). The distribution vector of the next generation will be called \(p_{t+1}\) and this way of creating a new generation will be called {\bf Random Distribution} (RD). In chapter 4 we will look into a different method, called Exact Distribution (EC), which distributes exactly \(p \cdot M\) '1's within the population. If no correction method (correction methods will be discussed later) is applied, it will simply be called {\bf No Correction} (NoC).\\

The probability for \(p_{t+1;i} = \frac{k_{i}}{M}\), i.e. the probability for generating a population with \(k_{i}\) '1's in the \(i\)th component on the basis of a given distribution vector \(p\), is
\begin{equation}
\label{eq:probabilityk}
P(p_{t+1;i} = \frac{k_{i}}{M}) = p^{k_{i}}_{t;i} (1 - p_{t;i})^{M - k_{i}} {M \choose k_{i}}
\end{equation}

\subsection{Expected Variance of a population}
We have on the one side the probability for the generation of a population with a certain \(k\) and a given \(p\) (equation~\ref{eq:probabilityk}) and the variance of such a population (equation~\ref{eq:variance4}). The expected variance \(d_p\) of the whole population on the basis of \(p\) and \(M\) is therefore the sum over all products of the variance and the probability of all values of \(k_{i}\):
\begin{equation}
\label{eq:totalv}
d_p = \sum_{i=1}^C \sum_{k_{i}=0}^{M} v_{k_{i}} P(p_{t+1;i} = \frac{k_{i}}{M}) = 
\sum_{i=1}^C \sum_{k_{i}=0}^{M} \frac{k_i M - k^{2}_{i}}{M^{2}} p^{k_{i}}_{i} (1 - p_{i})^{M - k_{i}} {M \choose k_{i}}
\end{equation}

\newpage
\section{Preventing variance loss with Random Distribution Correction (RDC)}

We will now examine a single component and how to prevent some of the variance loss that happens from one generation to the next. As previously stated in the scope of this report (UMDAs) each component is independent, i.e. \(C = 1\) and we set \(k = k_{i=1}\) and \(p = p_{i=1}\) for simplicity. The creation of one generation is as follows:

\begin{figure}[H]
\setbox0\vbox{\small
Given: Population of the previous generation $\Rightarrow$Variance \(v_{t}\)\\
\begin{enumerate}
\item Select \(N\) individuals and calculate \(p\)\\
\item Generate new population of size \(M\) on basis of \(p\) $\Rightarrow$Variance \(v_{t+1}\)\\
\end{enumerate}
}
\centerline{\fbox{\box0}}
\end{figure}

\subsection{Variance loss}

From \cite{Shapiro} we know that the variance loss of the two steps on a flat landscape with Random Distribution is \(1 - \frac{1}{N}\), i.e. (with \(v_{t}\) denoting the variance of the population of the last generation from which we have selected \(N\) individuals and \(v_{t+1}\) denoting the variance of the current population)

\begin{equation}
v_{t+1} = (1-\frac{1}{N})v_{t}
\end{equation}

When generating a new population on basis of \(p\) we learned in the last chapter (equation~\ref{eq:totalv}) that the total variance of the population that we will have after step (II) is \(d_{p}\). But the variance that we want is \(p ( 1 - p )\), i.e. the variance of a population of infinite size generated on basis of \(p\) and no variance loss. So the factor of variance loss of step (II) is \(\frac{d_p}{p(1-p)}\). Using for example the math program Maple this can be simplified to:
\begin{equation}
\label{eq:dcorrection}
\frac{d_p}{p(1-p)} = \frac{1}{p(1-p)} \sum_{k=0}^{M} \frac{k M - k^{2}}{M^{2}} p^{k} (1-p)^{M - k} {M \choose k} = {\bf 1 - \frac{1}{M}}
\end{equation}

So we have:
\begin{itemize}
\addtolength{\itemsep}{-0.5\baselineskip}
\item Factor of variance loss of step (II) : \(1 - \frac{1}{M}\) (from (\ref{eq:dcorrection})\\
\item Factor of Variance loss of step (I) and (II) together : \(1 - \frac{1}{N}\) (from \cite{Shapiro})\\
\end{itemize}

Our factor \(y\) of variance loss of step (I) is therefor: 
\begin{equation}
\label{eq:y}
1 - \frac{1}{N} = y \frac{d_p}{p(1-p)} \Leftrightarrow y = \frac{1 - \frac{1}{N}}{\frac{d_p}{p(1-p)}}
\end{equation}

With equation~(\ref{eq:dcorrection}) we get \(y = \frac{1 - \frac{1}{N}}{1 - \frac{1}{M}} = \frac{M(N-1)}{N(M-1)}\).\\

\subsection{Distribution vector correction}

If we use a different distribution vector \(q\) to generate the population and set \(q(1-q)\) (or \(q\) accordingly) in a way so that the theoretical factor of variance loss is 1 (i.e. no variance loss) we will reduce the overall variance loss. For now we will try to reduce the variance loss of only step (I), the variance loss of step (II) will be discussed in chapter 4.\\
As our variance loss in step (I) is \(y = \frac{M(N-1)}{N(M-1)}\) we have to multiply with the reciprocal value in order to nullify the variance loss and multiply that factor with our original \(p(1-p)\) variance. We are allowed to do that because the variance loss itself does not depend on the distribution vector we use:

\begin{equation}
x = \frac{1}{y} = \frac{N(M-1)}{M(N-1)}
\end{equation}

\begin{equation}
\label{eq:qpx}
q(1-q) = p(1-p) x \Leftrightarrow -q^2 + q - (-p^2 + p) x = 0 \Leftrightarrow q_{1/2} = \frac{1}{2} (1 \pm \sqrt{1 - 4 (-p^2 + p) x})
\end{equation}

For \(1 - 4 (-p^2 + p) \frac{N(M-1)}{M(N-1)} < 0\) we get a negative value in square root. The border values for p are:
\[
1 - 4 (-p^2 + p) \frac{N(M-1)}{M(N-1)} = 0 \Leftrightarrow
\]
\[
p_{1/2} = \frac{1}{2} (1 \pm \sqrt{1 - \frac{M(N-1)}{N(M-1)}})
\]
\\
\begin{wrapfigure}{r}[0cm]{5.5cm}
\includegraphics[scale=0.4]{graphs/graph_correction/graph_random_distribution_correction.png}
\mycaption{Correction graph for Distribution Correction}
\label{dc_correction_1}
\end{wrapfigure}

If our \(p\) is within \(p_{1}\) and \(p_{2}\)
\[
\frac{1}{2} (1 - \sqrt{1 - \frac{M(N-1)}{N(M-1)}}) < p < \frac{1}{2} (1 + \sqrt{1 - \frac{M(N-1)}{N(M-1)}})
\]
we will have to substitute with an appropriate value. As the function approaches \(0.5\) both from the top and from the bottom we will substitute with \(q = 0.5\) if our \(p\) is within those borders. In Figure \ref{dc_correction_1} you can see the graph of the correction function for a population size of 10. The green line represents \(f(p) = p\), i.e. no correction takes place. The larger the population size is the smaller is the horizontal line in the middle, i.e. the substitution with \(p = 0.5\).

\subsection{Code}
Finally we can put our results into a code, \(M\) is our population size \(N\) is our sampling size:\\
         
\begin{verbatim}
if( p < (1 - sqrt( 1 - (M*(N-1)) / (N*(M-1)) ) )/2 )
    q = (1 - sqrt( 1 - (N*(M-1)) / (M*(N-1)) * 4*p*(1-p) ) )/2;
else 
if( p > (1 + sqrt( 1 - (M*(N-1)) / (N*(M-1)) ) )/2 )
    q = (1 + sqrt( 1 - (N*(M-1)) / (M*(N-1)) * 4*p*(1-p) ) )/2;
else
    q = 1/2;
\end{verbatim}

It is easy to implement the code piece within an existing application as it is problem independent (within the restrictions described above, bit-strings on a flat landscape in UMDA). The code itself has to be inserted just after determining the distribution \(p\) from the selected part of the population of the size \(N\) and before generating the new population.

\newpage
\section{Exact Distribution (ED)}

So far we have calculated that the variance loss of generating a population on basis of \(p\) randomly (i.e. with RD) in step (II) will result in a variance loss of \(1 - \frac{1}{M}\) (equation~\ref{eq:dcorrection}). In order to create a new generation we originally set each value of each component of an individual of the new population to '1' with the probability of \(p\) (or to '0' with the probability of \(1-p\)). \\
We can further reduce the variance loss if we create the new generation by distributing exactly \(p \cdot M\) '1's (or \((1-p) M\) '0's) within the array of components in the population so that the new variance is exactly \(p (1 - p)\), i.e. there will be no variance loss in step (II). You can see that in Figure \ref{exact_distribution_1}, the methods using ED perform better in fitness and in diversity. You can also see that it has a lower convergence rate with simple problems like OneMax which is because with Random Distribution the population can change faster.\\

For this method our \(p\) is calculated this way:
\[
p = \frac{k_{old}}{N} \quad \mbox{and} \quad
k_{new} = \lfloor p M \rfloor = \left \lfloor \frac{k_{old} M}{N} \right \rfloor
\]

\(k_{old}\) denotes the number of '1's in the last generation, \(k_{new}\) denotes the number of '1's in the generation we are about to create. \(\lfloor p M \rfloor = \lfloor \frac{k_{old} M}{N} \rfloor\) does only apply when we do not change \(p\) through any correction method (like Laplace Correction).\\


\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_flat1163174282/graph_flat000_diversity.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163174291/graph_onemax000_fitness.png}
\mycaption{Comparison between Exact Distribution and Random Distribution}
\label{exact_distribution_1}
\end{figure}

\subsection{Exact Distribution Correction (EDC)}

When using an Exact Distribution the diversity loss in step (II) is expected to be different compared to the diversity loss of Random Distribution (\(1 - \frac{1}{M}\)). For now we will ignore the rounding error and assume that there are exactly \(p \cdot M\) '1's in the population. We have to change in equation~(\ref{eq:probabilityk}) the probability for \(\tilde{p_{i}} = \frac{k_{i}}{M}\) accordingly:

\[
\dot{P_(\tilde{p_{i}}} = \frac{k_{i}}{M}) = \left\{
\begin{array}{l l}
1 & \quad \mbox{for \(k_{i} = p_{i}M\)}\\
0 & \quad \mbox{for \(k_{i} \neq p_{i}M\)}\\
\end{array}
\right.
\]

The other equations (\ref{eq:variance1}), (\ref{eq:variance2}), (\ref{eq:variance3}), (\ref{eq:variance4}) about our variance \(v_{i}\) remain the same as they are dependent on our \(k_{i}\) anyways. Changes have to be made to equation~(\ref{eq:totalv}) because our \(P(\tilde{p_{i}} = \frac{k_{i}}{M})\) has changed. We no longer sum over all possible values of \(k\) as only (\(k = p \cdot M\)) has the probability '1' while all other values for \(k\) have the probability '0':

\begin{equation}
\dot{d}_p = \sum_{i=1}^C  v_{k_{i} = p_{i} M} P(\tilde{p_{i}} = \frac{k_{i} = p_{i}M}{M}) = 
\sum_{i=1}^C \frac{p_{i} M^{2} - (p_{i} M)^{2}}{M^{2}} = 
\sum_{i=1}^C p_{i} (1 - p_{i})
\end{equation}

As expected we have no loss of variance in step (II) using Exact Distribution because \(\frac{\dot{d}_p}{p(1-p)} = 1\). From chapter 3 we know that the expected variance loss in step (I)  is \(\frac{M(N-1)}{N(M-1)}\) (equation~\ref{eq:y}) which is now also our expected total variance loss from step (I) and (II) together. Using same the correction method from chapter 3 our expected variance loss should therefor be zero instead of \(1 - \frac{1}{M}\) as with the Random Distribution.
In Figure \ref{comparison_exact_1} you can see that compared with RDC from the previous chapter 'Exact Distribution Correction' has the lowest variance loss on a flat fitness landscape.
%TODO fitness?

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_flat1163174262/graph_flat000_diversity.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163174272/graph_onemax000_fitness.png}
\mycaption{Exact Distribution Correction has the lowest variance loss and reaches the highest fitness level}
\label{comparison_exact_1}
\end{figure}

\newpage
\subsection{Rounding errors}

The problem is that we actually cannot use the new distribution vector that we have calculated with EDC in order to generate the new population because we have demanded that \(p \cdot M\) is an integer.\\
When we simply calculate \(p\) by counting the number of '1's in the selected part of the population of size \(N\) such a rounding error will only occur if \(M\) is no multiple of \(N\). But if we change the distribution vector then our new \(p \cdot M\) will probably be no integer and we would have to change our EDC algorithm. So we have a mutual dependency between our algorithm and \(p\).\\
Thus, when using Exact Distribution, we either solve that dependency which is difficult to accomplish (see chapter 4.3). Or we ignore the error in our algorithm and either round to the nearest integer or, depending on the remainder, randomly decide whether to add another '1' (e.g. for \(p = 0.71\) and \(M = 10\) we would distribute seven '1's and an eighth '1' with the probability of \(0.1\)).\\
The drawback for the latter two methods is that the distribution no longer deserves the title 'exact' because we ignore the error from the ignored dependency although we still get a lower variance loss as seen in the tests. We will take a short look into the first method in the next section.

\subsection{Real Exact Distribution Correction}

As discussed in the previous section we cannot demand that our \(p \cdot M\) is an integer while changing the \(p\) with EDC as discussed earlier. Using the second method mentioned in the Rounding Errors section (randomly decide whether to generate one additional '1') in equation~(\ref{eq:probabilityk}) the probability for \(\tilde{p_{i}} = \frac{k_{i}}{M}\) obviously changes as there are \(\lfloor p \cdot M \rfloor\) '1's with the probability \(1-(p \cdot M - \lfloor p \cdot M \rfloor)\) and \(\lfloor p \cdot M \rfloor+1\) '1's with the probability \(p \cdot M - \lfloor p \cdot M \rfloor\):

\[
\dot{P}_(\tilde{p_{i}} = \frac{k_{i}}{M}) = \left\{
\begin{array}{l l}
1-(p M-\lfloor p M \rfloor) & \quad \mbox{for \(k_{i} = \lfloor p_{i} M \rfloor\)}\\
p M-\lfloor p M \rfloor & \quad \mbox{for \(k_{i} = \lfloor p_{i} M \rfloor+1\)}\\
0 & \quad \mbox{else}\\
\end{array}
\right.
\]

Again, the equations about our variance \(v_{k_{i}}\) remain the same and changes have to be made to equation~(\ref{eq:totalv}) because our \(P\tilde{p_{i}} = \frac{k_{i}}{M})\) has changed and we no longer sum over all possible values of \(k\) but only two:

\[
\dot{d}_p = \sum_{i=1}^C  \sum_{k_{i}=0}^{M} v_{k_{i}} P(\tilde{p_{i}} = \frac{k_{i}}{M}) = 
\]
\[
            \sum_{i=1}^C                     v_{k_{i} = \lfloor p_{i} M \rfloor}   P(\tilde{p_{i}} = \frac{k_{i} = \lfloor p_{i} M \rfloor}{M}) +
                                             v_{k_{i} = \lfloor p_{i} M \rfloor+1} P(\tilde{p_{i}} = \frac{k_{i} = \lfloor p_{i} M \rfloor+1}{M}) =
\]
\[
            \sum_{i=1}^C                     v_{\lfloor p_{i} M \rfloor}   (1-(p M-\lfloor p M \rfloor))
                                             v_{\lfloor p_{i} M \rfloor+1}    (p M-\lfloor p M \rfloor) =
\]
\[
	    \sum_{i=1}^C  \frac{\lfloor p_{i} M \rfloor M - \lfloor p_{i} M \rfloor^{2}}{M^{2}} (1-(p M-\lfloor p M \rfloor)) +
			  \frac{(\lfloor p_{i} M \rfloor+1) M - (\lfloor p_{i} M \rfloor+1)^{2}}{M^{2}} (p M-\lfloor p M \rfloor) =
\]
\[
	    \sum_{i=1}^C  \frac{
				(\lfloor p_{i} M \rfloor)^2 - 2 \lfloor p_{i} M \rfloor p M + p M^{2} - p M + \lfloor p M \rfloor}
				{M^{2}
			       }
\]

Our variance loss is as before \(\frac{\dot{d}_p}{p (1-p)}\) which unfortunately cannot be simplified any further and therefore is dependent on \(p\). With equation~(\ref{eq:y}) we have 
\[
y = \frac{1 - \frac{1}{N}}{\frac{\dot{d}_p}{p (1-p)}} \mbox{\quad and \quad} x = \frac{1}{y} = \frac{\frac{\dot{d}_p}{p (1-p)}}{1 - \frac{1}{N}}
\]

The problem is that we cannot easily insert that into equation~(\ref{eq:qpx}) as in this case our \(x\) depends on the distribution vector \(p\) with which we create the new population. This is a problem because the creation of our new population in step (II) happens of course {\emph after} the correction of \(p\). So we really have to make \(x\) dependent on the distribution vector \(\dot{q}\):

\[
\dot{q}(1-\dot{q}) = p(1-p) x_{\dot{q}}
\]
This is kind of difficult to solve for \(\dot{q}\) and probably has to be solved numerically. On the one hand this 'corrected' correction would further increase the effectiveness of the erroneous 'Exact Distribution Correction' discussed earlier while on the other hand it would cost more calculation time. Additional tests have to be made to determine if it is worth it.

\newpage
\section{Problem types}

\subsection{Flat fitness landscape}

In this test we examine the behavior of the algorithms in terms of their {\emph diversity} with varying parameters on a flat fitness landscape. It is basically a 'needle in a haystack' problem where the needle is not found within the 200 generations, i.e. all solutions have the same fitness. The problem size is a bit-string with length 10, i.e. we have 10 components.\\
In Figure \ref{flat_fitness_landscape} we can see the diversity loss of several different methods that were defined earlier. The additional graph, '1 - 1/N loss', denotes the expected loss of diversity per generation according to \cite{Shapiro} with Random Distribution, 'M(N-1)/(N(M-1)) loss' denotes the expected loss of diversity that we have shown in chapter 4 with Exact Distribution. You can see that a test run of both Random and Exact Distribution match approximately the loss that was expected.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_flat1163449155/graph_flat000_diversity.png}
\mycaption{Diversity loss on a flat fitness landscape (population size 10)}
\label{flat_fitness_landscape}
\end{figure}

\subsection{OneMax problem}

\begin{wrapfigure}{r}[0cm]{6.5cm}
\vspace{-0.7in}
\includegraphics[scale=0.4]{graphs/graph_onemax_fitness_landscape.png}
\mycaption{Fitness Landscape of OneMax}
\label{fitness_landscape_onemax}
\end{wrapfigure}
In this test we examine the behaviour of the algorithms in terms of their {\emph fitness} with varying parameters with the problem OneMax. The goal is to find the string '111\dots', each '1' in a component of an individual gets rewarded by one fitness point. This is also the main difference to a flat fitness landscape, when selecting individuals for a new generation we do not randomly take \(N\) individuals and calculate our distribution vector \(p\) but we sort all individuals by their fitness and only select the top 50\%.\\
 While the OneMax problem does not represent a problem with a real flat fitness landscape (thus the theory we discussed earlier does not apply here), a local flat fitness landscape can occur if the variance drops and/or the fitness values of the population are very similar (e.g. 010, 100, 001).\\
In our test the problem size ranges from 100 to 500 depending on the convergence speed of the functions that we will examine.\\

\subsection{Leading-1s problem}

In the Leading-1s problem the fitness is calculated by counting the number of leading '1's. This implies that contrary to OneMax the bit-position is important and the bits are connected, i.e. component \(i\)'s fitness depends on all components \(j\) with \(0 < j < i\). This is more difficult to handle for UMDA's discussed in this report.\\
For this problem keeping correct solutions on lower bit-positions is very important, i.e. lower bit-positions contribute to the total fitness more than higher bit-positions. Any algorithm that maintains a high diversity in positions that already have the correct value will have a significantly lower fitness.

\subsection{NK Landscape}

From \cite{NKLandscape}:

,,An NK landscape is a real-valued function defined on the set of binary \(n\)-tuples, \(\{0,1\}^{n}\), which is of the form

\[
f (x_1, x_2, \dots, x_n) = \sum_{i=1}^{n} f_{i} (x_i, \prod (x_i))
\]

It is a summation of local fitness functions \(f_{i}\)'s, where each \(f_{i}\) depends on its main variable \(x_{i}\) and the variables in the neighborhood of \(x_{i}\). Here the neighborhood \(\prod (x_{i})\) is a subset of the set \(\{x_{1}, x_{2}, \dots, x_{n}\} \backslash \{x_{i}\}\) and its size \(|\prod_{k}(x_{i})|\) is \(k\). There are two ways to choose the variables in the neighborhood \(\prod (x_{i})\), adjacent neighborhood and random neighborhood. In the NK models with adjacent neighborhood, \(\prod(x_{i})\) consists of the closest \(k\) variables (with a certain to-break) to the main variable \(x_{i}\) with respect to the indices modulo \(n\). In the NK models with random neighborhood, \(\prod(x_{i})\) is composed of the \(k\) variables chosen uniformly at random from \(\{x_{1}, x_{2}, \dots, x_{n}\} \backslash \{x_{i}\}\). Local fitness functions are constructed independently of each other. For each local fitness function, a random value from a probability distribution is assigned for each input.''\\

We will use a similar implementation as \cite{MitchelPotter}, with 'Random(\(x\))' denoting a function that maps \(x\) to a random value. 
\[
f_{i} = \frac{\mbox{Random}(2^{k})}{2^{k}}
\]

That means that two function calls with the same parameter (i.e. the same bit pattern) will result in the same value. If a new individual is created with the same pattern it gets the same fitness for this pattern. For \(k = 1\) we would get a similar function as OneMax as there are only two possible values for \(f_{i}\), i.e. we are counting '1's (or '0's). For large values of \(k\) this means that a large portion of memory is needed at the beginning (\(2^{k}\) e.g. 4 GB for \(k = 32\)) so for the implementation we will use a hashtable and relative small numbers (\(k = 10\) and \(n = 50\)).

\newpage
\section{Additional methods}

\subsection{Boundary Correction (BC)}
\begin{wrapfigure}{r}[0cm]{5.5cm}
\vspace{-0.7in}
\includegraphics[scale=0.4]{graphs/graph_correction/graph_random_distribution_boundary_correction.png}
\mycaption{Correction graph for Boundary Correction}
\label{bd_correction_1}
\end{wrapfigure}

An optional correction method is called {\bf Boundary Correction}. It can be used in connection with NoC, RDC and EDC and checks the boundaries of the final value of the distribution vector \(p\). In the case \(p\) gets above \(1 - \beta\) or below \(\beta\) it is corrected to these boundaries:

\[
p = \left\{
\begin{array}{l l}
\beta & \quad \mbox{for \(p < \beta\)}\\
1 - \beta & \quad \mbox{for \(p > 1 - \beta\)}\\
p & \quad \mbox{else}\\
\end{array}
\right.
\]

\subsection{Laplace Correction (LC)}	

\begin{wrapfigure}{r}[0cm]{5.5cm}
\vspace{-0.7in}
\includegraphics[scale=0.4]{graphs/graph_correction/graph_laplace_correction.png}
\mycaption{Correction graph for Laplace Correction}
\label{lc_correction_1}
\end{wrapfigure}

Laplace correction is (so far) more or less the standard in connection with UMDA. It biases the resulting \(p\) towards \(\frac{1}{2}\) and ensures that components do not get stuck at \(p = 1.0\) or \(p = 0.0\). Basically it behaves very much like Boundary Correction except that it cannot be combined with another correction method. The general formula for Laplace correction for a component \(i\) is to divide the number of '1's in the selected population plus \(\alpha\) by the size of the selected population plus \(2\alpha\).
\begin{equation}
p_{i} = \frac{k_{i} + \alpha}{N + 2\alpha}
\end{equation}


\subsection{Laplace Remember Correction (LRC)}

\begin{wrapfigure}{r}[0cm]{5.5cm}
\vspace{-0.7in}
\includegraphics[scale=0.4]{graphs/graph_correction/graph_laplace_remember_correction.png}
\mycaption{Correction graph for Laplace Remember Correction with \(p_{t} = 0.25\)}
\label{lrc_correction_1}
\end{wrapfigure}

Another option is to include the distribution vector of the previous generation. We will replace \(\alpha\) in the numerator with \(\tilde{\alpha} = 2 p_{t;i} \alpha\) where \(p_{t+1}\) denotes the new and \(p_{t}\) denotes the old distribution vector. In Figure \ref{lrc_correction_1} you can see that the function changes the new \(p\) to a lower value if the \(p_{t}\) was lower than \(0.5\). Effectively this means that we soften sudden changes between two generations by a small degree.
\[
p_{t+1;i} = \frac{k_{i} + 2 \alpha p_{t;i}}{N + 2 \alpha}
\]

While including previous generations into the generation is not the scope of this report, it is certainly worth further examination. For example we can combine EDC with LRC by first executing EDC and then LRC. See chapter 8 for a more detailed analysis.

\newpage
\section{General test configuration}

\subsection{Graph drawing}

The graphs were created with the help of gnuplot 4.0 and g++ 4.0 (gnu C++), the actual program is available on CD-ROM. 50 separate runs were made for each parameter setting and algorithm because all algorithms are based on random numbers. In addition each run consists of a number of generations. In each generation the best fitness value that was found so far in that run is recorded. Then for each generations all recorded values are averaged between the runs and drawn as a graph. I.e. if we have one run (1 1 2 5 5 8) and another run (2 2 2 3 3 5) then our resulting graph would be (1.5 1.5 2 4 4 6.5).

\subsection{Population size}

Depending on the problem type, problem size and algorithm different population sizes ranging from 10 to 100 were tested. Also for each single configuration several different population sizes were tested because the convergence speed differ between the methods. Larger populations automatically result in more diversity and less diversity loss in each generation.

\subsection{Selection size N}

For problem types that do not use a flat fitness landscape we are interested in their fitness. We usually select the top 50\% from the population in order to calculate the distribution vector \(p\). Higher selection rates will reduce the convergence speed because we are including below average solutions in \(p\) while lower selection rates will significantly increase the diversity loss. If the convergence speed is reduced the method will have difficulties to reach or maintain good solutions either because the diversity dropped already to zero or (with Boundary Check or Laplace Correction) the high diversity holds back good solutions from spreading through the population. It is not the scope of this report to discuss this subject in detail but to present the general effect of different selection sizes.\\

In Figure \ref{selectionsize1} we can see the trade-off between speed and quality and that elitism (i.e. small selection size) with methods that do not prevent uniformity in the population (i.e. no Boundary Correction or Laplace Correction) is very bad. With OneMax a selection size of 80\% (or even higher) is the best choice because a high selection size simply maintains the diversity slightly longer above zero.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_leading1163450163/graph_leading000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163450637/graph_onemax000_fitness.png}
\mycaption{Different selection sizes without boundary correction (LeadingOnes and OneMax, population size 50)}
\label{selectionsize1}
\end{figure}

We can see this also with more difficult problems like the NK landscape (Figure \ref{selectionsize2}), any extreme value for selection size generally does not improve the search.

%pop10 und pop30
\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_nk1163198367/graph_nk000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163198367/graph_nk001_fitness.png}
\mycaption{Different selection sizes {\bf without} boundary correction (NK-Landscape, population size 10 and 30)}
\label{selectionsize2}
\end{figure}


On the other hand we can see in Figure \ref{selectionsize4} that elitism clearly pays off when using Boundary Correction with easier problems like LeadingOnes while being below average with more difficult problems like the NK landscape (Figure \ref{selectionsize3}) or when using no Boundary Correction. So we can conclude that generally a selection size of 50\% is a good standard value.

%pop10 pop30
\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_nk1163198192/graph_nk000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163198192/graph_nk001_fitness.png}
\mycaption{Different selection sizes {\bf with} boundary correction (NK-Landscape, population size 10 and 30)}
\label{selectionsize3}
\end{figure}

%pop30 und pop30
\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_leading1163208885/graph_leading001_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_leading1163209153/graph_leading001_fitness.png}
\mycaption{Different selection sizes with and without boundary correction (LeadingOnes, population size 30)}
\label{selectionsize4}
\end{figure}

\newpage
\section{Discussion and comparison of the correction methods}

\subsection{Boundary Correction}

\begin{wrapfigure}{r}[0cm]{5.5cm}
\hspace{-0.5in}
\vspace{-0.7in}
\includegraphics[scale=0.4]{graphs/graph_flat1163209833/graph_flat000_diversity.png}
\mycaption{With boundary correction the diversity is held at a certain level (Flat fitness landscape)}
\label{bd_correction_2}
\end{wrapfigure}

You can see in Figure \ref{bd_correction_2} that with BC the diversity is held at a certain level. For each component there is always at least a chance of \(\beta\) to switch between '0' and '1'. On the one side this has the positive effect that theoretically a better or even the optimal solution will be found. On the other hand single components that already contain an optimal value can switch back to the wrong value.\\

If \(\beta\) is too large the function will have problems preserving the solution, if \(\beta\) is too low the function might need much more time to get out of a local optimum. For example in the case of an OneMax problem to get from 99 '1's to 100 '1's we not only need to generate another '1' but we have to preserve all the other 99 '1's, too. Thus we can conclude that a good value for \(\beta\) is dependent on all parameters, the population size, the problem type, the correction type and especially the problem size. In addition we can conclude that a very low value (relative to the problem size) for \(\beta\) will always be positive in the long run while reducing its convergence speed a little. In Figure \ref{bd_correction_3} you can see that there values for \(\beta\) with which EDC+BC clearly is better than EDC without BC (i.e. \(\beta = 0\)) and in Figure \ref{bd_correction_4} that the optimal value depends on the population size.\\

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_onemax1163211796/graph_onemax000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163211796/graph_onemax000_diversity.png}
\mycaption{Different values for \(\beta\) (OneMax, population size 10)}
\label{bd_correction_3}
\end{figure}

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_nk1163212098/graph_nk000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163212098/graph_nk001_fitness.png}
\mycaption{Different values for \(\beta\) (NK-Landscape, population size 10 and 30)}
\label{bd_correction_4}
\end{figure}

Comparing EDC again with ED, this time with Boundary Correction, we see (Figure \ref{bd_correction_5}) that with simple problems like OneMax using EDC+BC will result in worse fitness values. This is because EDC+BC maintains the diversity level at a higher level than ED+BC which makes it impossible to go beyond a certain number of '1's in the population.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_onemax1163451007/graph_onemax000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163451007/graph_onemax000_diversity.png}
\mycaption{The variance that EDC+BC maintains is too large in comparison with ED+BC (OneMax, population size 10)}
\label{bd_correction_5}
\end{figure}

When combining Boundary Correction with either RDC or EDC we will face the same problem as discussed with Exact Distribution. When we change our \(p\) before we create our population we will also have to adapt the Correction methods themselves. This could be a small source of error and probably have to be researched more closely if there is an algorithm that performs better than RDC+BC or EDC+BC by including \(\beta\) in its calculation.\\

\subsection{Laplace Correction}

Again the optimal value for \(\alpha\) depends on the type of the problem and the problem parameters. As with Boundary Correction there can be a trade-off between convergence speed and fitness for some problems as we can see in Figure \ref{laplace_alpha_1}. If we set \(\alpha\) very low we have a chance to get out of uniformity of a component but a low convergence speed. If we use higher values for \(\alpha\) we get faster to better solutions but are no longer able to keep good solutions.\\
%pop30
\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_onemax1163455064/graph_onemax000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_leading1163455185/graph_leading000_fitness.png}
\mycaption{\(\alpha\) influences the effectiveness of Laplace correction but is problem-dependent, (OneMax and LeadingOnes, population size 30)}
\label{laplace_alpha_1}
\end{figure}

\subsection{Boundary vs. Laplace Correction}

As we have discussed earlier, both Laplace Correction and Boundary Correction do prevent \(p\) from getting near the boundary values \(p = 0.0\) and \(p = 1.0\). In addition LC biases the resulting \(p\) to \(\frac{1}{2}\) due to the lower gradient. We can expect that Laplace Correction outperforms a simple Boundary Correction in terms of variance on a flat fitness landscape, the question is if this bias is significant.\\
For the test we will look at configurations of \(\alpha\) and \(\beta\) that change \(p\) at the borders to the same values. Laplace Correction corrects \(p = 0\) to \(\frac{\alpha}{N + 2\alpha}\), Boundary Check corrects \(p = 0\) to \(\beta\), so in order to compare the behaviour at the borders we set \(\beta = \frac{\alpha}{N + 2\alpha}\). In Figure \ref{boundedvslaplace1} we have set \(\alpha = 0.1\) and \(beta = 0.0193\) (for N = 5). As expected LC provides more variance but as we can see in the fitness graph at the cost of a lower convergence speed. 

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_onemax1163347837/graph_onemax000_diversity.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163347837/graph_onemax000_fitness.png}
\mycaption{Comparison of diversity and fitness (OneMax, population size 10)}
\label{boundedvslaplace1}
\end{figure}

With a more difficult problem like a NK-landscape we see that LC again maintains a higher level of diversity which causes it to be slower at the beginning but reaching a higher level of fitness later (Figure~\ref{boundedvslaplace3}).
\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_nk1163349298/graph_nk000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163349298/graph_nk000_diversity.png}
\mycaption{Comparison of fitness and diversity (NK-Landscape, population size 10)}
\label{boundedvslaplace3}
\end{figure}

In conclusion we can say that using Laplace Correction is not much different than Boundary Correction. Due to the lower gradient of the correction function it has a lower convergence rate which allows the method to search a longer in a population with higher diversity at the beginning. The main disadvantage of LC is that we can not easily combine it with other correction functions. Boundary Correction on the other hand only changes the borders of the correction function and will be used in this paper in connection with all other methods.

\subsection{Laplace Remember Correction}

Tests have shown that the value for \(\alpha\) is not significant (see Figure \ref{laplace_remember_alpha_1}) so we set \(\alpha = 1.0\). As the method does not prevent extreme values for \(p\) we will also use a Boundary Correction in connection with LRC. 

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_flat1163339386/graph_flat000_diversity.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163339355/graph_onemax000_fitness.png}
\mycaption{No significant difference between different values of \(\alpha\) in diversity loss and fitness (Flat fitness landscape and OneMax, population size 10)}
\label{laplace_remember_alpha_1}
\end{figure}

LRC behaves similarly as the underlying distribution function as can seen in Figure \ref{laplace_remember_2}. Thus LRC alone does not give have any real advantage but we section we will try to combine LRC with EDC.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_leading1163345629/graph_leading000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163345894/graph_nk000_fitness.png}
\mycaption{Similar behavior of LRC as the underlying distribution method (Leading Ones and NK-Landscape, population size 10)}
\label{laplace_remember_2}
\end{figure}


\subsection{EDC with Laplace Remember Correction}

As stated earlier we can combine EDC with LRC by first executing EDC and then LRC with the parameters \(p_{t;i}\) and \(k_{i} = q_{i} \cdot N\), with \(q_{i}\) being the distribution vector corrected by EDC. Preventing sudden changes might be useful in connection with the Distribution Correction because the correction graph (Figure \ref{dc_correction_1}) has a large gradient left of \(p_{1}\) and right of \(p_{2}\).\\
In Figure \ref{laplace_remember_edc_1} we can see that LRC does provide a higher level of diversity on a flat fitness landscape and that it performs slightly better with the OneMax problem at the cost of a lower convergence speed.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_flat1163359845/graph_flat000_diversity.png}
\includegraphics[scale=0.5]{graphs/graph_onemax1163443259/graph_onemax000_fitness.png}
\mycaption{Comparison between EDC+LRC and EDC in terms of their diversity and fitness (Flat fitness landscape and Onemax, population size 30)}
\label{laplace_remember_edc_1}
\end{figure}

While some runs show a better fitness by a small margin, others do not (Figure \ref{laplace_remember_edc_2}. Thus it is problem dependent if LRC improves the method.

\begin{figure}[H]
\hspace{-0.5in}
\includegraphics[scale=0.5]{graphs/graph_nk1163444358/graph_nk000_fitness.png}
\includegraphics[scale=0.5]{graphs/graph_nk1163444572/graph_nk000_fitness.png}
\mycaption{LRC does not necessarily improve fitness, only for some problem instances (NK landscape, same parameters, population size 20)}
\label{laplace_remember_edc_2}
\end{figure}

\newpage

\section{Summary}

\section{Fields of further research and summary}

There are several open points in this report that were not discussed or left open for further research. Firstly the EDC can be improved by including the rounding error in the calculation. As the distribution vector \(p\) can hold only a limited number of values (\(N+1\) values) prior to the correction it is certainly possible to solve this numerically with a limited accuracy.\\
In addition the complete proof for equation~\ref{eq:dcorrection} is missing and is only demonstrated by Maple to be correct.\\
Also there is missing a rule to determine the optimal values for \(\beta\) in the Boundary Correction method in connection with the parameters (problem size/type and population size). It was demonstrated that the results can be influenced very negatively when choosing too large values for \(\beta\).\\
Another point is that, if applied in a real application, the calculation time maybe another important factor. It was shown that EDC performed better than ED but only by a relatively small margin. EDC needs another calculation step and a table access respectively and needs more generations to reach the (higher) convergence level so it might be slower altogether.\\
Although it reaches sometimes a lower fitness level, LRC in connection with EDC might be the answer concerning the convergence speed and fitness trade-off. It needs more research to determine how to use the distribution vectors of previous generations effectively.\\
At last other forms of EDAs where a dependency between the components is included in the calculation need to be researched using similar methods of correcting their diversity loss as demonstrated in this report with UMDAs.\\

\newpage
\begin{thebibliography}{99}
\bibitem{Shapiro} {\sc Shapiro, J.L.:}  \textit{Diversity loss in general estimation of distribution algorithms}, 2006.
\bibitem{Muehlenbein} {\sc H. Muehlenbein and G. Paasz:}  \textit{From recombination of genes to the estimation of distributions: I. binary parameters.}, H.-M. Voigt, W. Ebeling, I. Rechenberg, and H.-P. Schwefel, editors, Parallel Problem Solving from Nature-PPSN IV, pages 178-187, Berlin, 1996. Springer.
\bibitem{NKLandscape} {\sc Y. Gao and J. Culberson:}  \textit{An Analysis of Phase Transition in NK Landscapes}, \url{http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume17/gao02a.ps.Z}
\bibitem{MitchelPotter} {\sc Potter, M.A.:}  \textit{NK-landscape problem generator}, \url{http://web.archive.org/web/20011120064244/cs.gmu.edu/~mpotter/nk-generator/} or \url{http://www.cs.uwyo.edu/~wspears/nk.c}

\end{thebibliography}
\end{document}

